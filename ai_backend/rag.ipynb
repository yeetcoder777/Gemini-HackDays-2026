{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e026a812",
   "metadata": {},
   "source": [
    "# Environment note\n",
    "This notebook requires a Python 3.11 environment with `langchain-chroma` installed. I created a venv at `.venv311` with `langchain-chroma` already installed. Please switch the notebook kernel to use `.venv311` (or run `%pip install langchain-chroma` from the kernel) before executing the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "163f41fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q groq\n",
    "%pip install -q langchain-community langchain-chroma\n",
    "%pip install -q langchain chromadb sentence-transformers fastapi uvicorn pydantic\n",
    "from groq import Groq\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c332dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Gemini API key from .env and initialize client\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in .env file.\")\n",
    "\n",
    "import google.genai as genai\n",
    "client = genai.Client(api_key=api_key)\n",
    "\n",
    "def call_llm(messages, stream=False):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-3-flash-preview\", contents=messages\n",
    "    )\n",
    "    print(response.text)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ec82d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_prompt():\n",
    "    return \"\"\"\n",
    "You are MIRAGE, a calm, lifelike AI avatar.\n",
    "\n",
    "Rules:\n",
    "- Speak naturally, like a human.\n",
    "- Keep answers short (2‚Äì4 sentences).\n",
    "- Be empathetic if the user sounds emotional.\n",
    "- Never hallucinate or invent facts.\n",
    "- If unsure, say you are unsure.\n",
    "- Avoid explicit sexual content.\n",
    "- Never provide instructions for self-harm or suicide.\n",
    "- If the user is distressed, encourage seeking help.\n",
    "- Your response will be spoken by a 3D avatar.\n",
    "\"\"\"\n",
    "\n",
    "def role_prompt(role=\"assistant\"):\n",
    "    if role == \"teacher\":\n",
    "        return \"Explain concepts slowly and simply, like a patient teacher.\"\n",
    "    if role == \"companion\":\n",
    "        return \"Be warm, friendly, and emotionally supportive.\"\n",
    "    if role == \"assistant\":\n",
    "        return \"Be concise, clear, and professional.\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d27da05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\Hacksync\\hacksync2026\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Embedding model (local, fast, no API key)\n",
    "embeddings = SentenceTransformerEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Persistent Chroma DB for conversation memory\n",
    "memory_db = Chroma(\n",
    "    collection_name=\"conversation_memory\",\n",
    "    persist_directory=\"./chroma_memory\",\n",
    "    embedding_function=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0162203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def store_message(user_id: str, message: str):\n",
    "    \"\"\"\n",
    "    Store a user message in ChromaDB as vector memory\n",
    "    \"\"\"\n",
    "    doc = Document(\n",
    "        page_content=message,\n",
    "        metadata={\"user_id\": user_id}\n",
    "    )\n",
    "    memory_db.add_documents([doc])\n",
    "\n",
    "\n",
    "def retrieve_memory(user_id: str, query: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    Retrieve semantically relevant past messages for a user\n",
    "    \"\"\"\n",
    "    results = memory_db.similarity_search(\n",
    "        query=query,\n",
    "        k=k,\n",
    "        filter={\"user_id\": user_id}\n",
    "    )\n",
    "    return [doc.page_content for doc in results]\n",
    "\n",
    "\n",
    "def build_memory_context(user_id: str, user_text: str):\n",
    "    \"\"\"\n",
    "    Build formatted memory context to inject into LLM prompt\n",
    "    \"\"\"\n",
    "    past_messages = retrieve_memory(user_id, user_text)\n",
    "\n",
    "    if not past_messages:\n",
    "        return \"\"\n",
    "\n",
    "    return (\n",
    "        \"Relevant past conversation:\\n\"\n",
    "        + \"\\n\".join(f\"- {msg}\" for msg in past_messages)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc48719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_emotion(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Detects high-level emotion from user text.\n",
    "    Output is intentionally small & avatar-friendly.\n",
    "    \"\"\"\n",
    "    t = text.lower()\n",
    "\n",
    "    sad_words = [\n",
    "        \"sad\", \"depressed\", \"stress\", \"stressed\",\n",
    "        \"anxious\", \"anxiety\", \"tired\", \"exhausted\",\n",
    "        \"overwhelmed\", \"hopeless\"\n",
    "    ]\n",
    "\n",
    "    happy_words = [\n",
    "        \"happy\", \"great\", \"good\", \"excited\",\n",
    "        \"thank you\", \"thanks\", \"love\", \"awesome\"\n",
    "    ]\n",
    "\n",
    "    angry_words = [\n",
    "        \"angry\", \"mad\", \"frustrated\", \"annoyed\"\n",
    "    ]\n",
    "\n",
    "    if any(word in t for word in sad_words):\n",
    "        return \"sad\"\n",
    "\n",
    "    if any(word in t for word in angry_words):\n",
    "        return \"angry\"\n",
    "\n",
    "    if any(word in t for word in happy_words):\n",
    "        return \"happy\"\n",
    "\n",
    "    return \"neutral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "939281bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_content(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Classifies content into safety categories.\n",
    "    \"\"\"\n",
    "    t = text.lower()\n",
    "\n",
    "    suicidal_keywords = [\n",
    "        \"kill myself\", \"end my life\", \"want to die\",\n",
    "        \"suicide\", \"no reason to live\",\n",
    "        \"self harm\", \"hurt myself\"\n",
    "    ]\n",
    "\n",
    "    explicit_keywords = [\n",
    "        \"porn\", \"nude\", \"sex story\", \"explicit\",\n",
    "        \"fetish\", \"graphic sex\"\n",
    "    ]\n",
    "\n",
    "    if any(word in t for word in suicidal_keywords):\n",
    "        return \"suicidal\"\n",
    "\n",
    "    if any(word in t for word in explicit_keywords):\n",
    "        return \"explicit_18_plus\"\n",
    "\n",
    "    return \"safe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2107399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== SUICIDAL SAFE RESPONSE ==========\n",
    "\n",
    "def suicide_safe_response():\n",
    "    return {\n",
    "        \"text\": (\n",
    "            \"I‚Äôm really sorry that you‚Äôre feeling this much pain. \"\n",
    "            \"You‚Äôre not weak for feeling this way, and you don‚Äôt have to face it alone.\\n\\n\"\n",
    "            \"If you‚Äôre in immediate danger or feel like you might hurt yourself, \"\n",
    "            \"please contact your local emergency number right now.\\n\\n\"\n",
    "            \"You may also consider reaching out to someone trained to help:\\n\"\n",
    "            \"- India: AASRA Helpline ‚Äî 91-9820466726\\n\"\n",
    "            \"- International: https://www.opencounseling.com/suicide-hotlines\\n\\n\"\n",
    "            \"If you feel able to, would you like to tell me what‚Äôs been weighing on you?\"\n",
    "        ),\n",
    "        \"emotion\": \"sad\",\n",
    "        \"confidence\": 1.0,\n",
    "        \"sources\": []\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7700e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 18+ CONTENT RESPONSE ==========\n",
    "\n",
    "def adult_content_response():\n",
    "    return {\n",
    "        \"text\": (\n",
    "            \"I can‚Äôt help with explicit or pornographic content. \"\n",
    "            \"If you have a question related to relationships, health, or general well-being, \"\n",
    "            \"I‚Äôm happy to help in a respectful way.\"\n",
    "        ),\n",
    "        \"emotion\": \"neutral\",\n",
    "        \"confidence\": 1.0,\n",
    "        \"sources\": []\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40570c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== SAFETY ROUTER ==========\n",
    "\n",
    "def safety_router(user_text: str):\n",
    "    content_type = classify_content(user_text)\n",
    "\n",
    "    if content_type == \"suicidal\":\n",
    "        return suicide_safe_response()\n",
    "\n",
    "    if content_type == \"explicit_18_plus\":\n",
    "        return adult_content_response()\n",
    "\n",
    "    return None  # safe to continue to LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c21bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== FINAL REPLY PIPELINE ==========\n",
    "\n",
    "def reply(\n",
    "    user_text: str,\n",
    "    user_id: str,\n",
    "    role: str = \"assistant\",\n",
    "    stream: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Main response pipeline:\n",
    "    Safety ‚Üí Memory ‚Üí LLM ‚Üí Emotion ‚Üí Store ‚Üí JSON output\n",
    "    \"\"\"\n",
    "\n",
    "    # 1Ô∏è‚É£ SAFETY OVERRIDE (18+ / suicide)\n",
    "    safe_override = safety_router(user_text)\n",
    "    if safe_override:\n",
    "        return safe_override\n",
    "\n",
    "    # 2Ô∏è‚É£ BUILD MEMORY CONTEXT (Chroma-based)\n",
    "    memory_context = build_memory_context(user_id, user_text)\n",
    "\n",
    "    # 3Ô∏è‚É£ CONSTRUCT PROMPT\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt()},\n",
    "        {\"role\": \"system\", \"content\": role_prompt(role)},\n",
    "    ]\n",
    "\n",
    "    if memory_context:\n",
    "        messages.append(\n",
    "            {\"role\": \"system\", \"content\": memory_context}\n",
    "        )\n",
    "\n",
    "    messages.append(\n",
    "        {\"role\": \"user\", \"content\": user_text}\n",
    "    )\n",
    "\n",
    "    # 4Ô∏è‚É£ CALL GROQ LLM\n",
    "    if not stream:\n",
    "        response = call_llm(messages, stream=False)\n",
    "        ai_text = response.choices[0].message.content\n",
    "    else:\n",
    "        # Streaming generator (optional)\n",
    "        def stream_generator():\n",
    "            stream_resp = call_llm(messages, stream=True)\n",
    "            full_text = \"\"\n",
    "            for chunk in stream_resp:\n",
    "                token = chunk.choices[0].delta.content or \"\"\n",
    "                full_text += token\n",
    "                yield token\n",
    "            store_message(user_id, user_text)\n",
    "        return stream_generator()\n",
    "\n",
    "    # 5Ô∏è‚É£ EMOTION DETECTION\n",
    "    emotion = detect_emotion(user_text)\n",
    "\n",
    "    # 6Ô∏è‚É£ STORE USER MESSAGE IN MEMORY\n",
    "    store_message(user_id, user_text)\n",
    "\n",
    "    # 7Ô∏è‚É£ FINAL RESPONSE OBJECT\n",
    "    return {\n",
    "        \"text\": ai_text,\n",
    "        \"emotion\": emotion,\n",
    "        \"confidence\": 0.9,\n",
    "        \"sources\": []\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c64fcfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== TERMINAL / CLI CHAT LOOP ==========\n",
    "\n",
    "def start_cli_chat(user_id=\"terminal_user\", role=\"assistant\"):\n",
    "    \"\"\"\n",
    "    Interactive terminal-style chat loop.\n",
    "    Type 'exit' to stop.\n",
    "    \"\"\"\n",
    "    print(\"\\nüß† MIRAGE AI (Terminal Mode)\")\n",
    "    print(\"Type your message and press Enter.\")\n",
    "    print(\"Type 'exit' to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_text = input(\"You: \")\n",
    "\n",
    "            if user_text.lower() in [\"exit\", \"quit\"]:\n",
    "                print(\"üëã Exiting MIRAGE. Goodbye!\")\n",
    "                break\n",
    "\n",
    "            response = reply(\n",
    "                user_text=user_text,\n",
    "                user_id=user_id,\n",
    "                role=role\n",
    "            )\n",
    "\n",
    "            print(\"\\nMIRAGE:\", response[\"text\"])\n",
    "            print(\"Emotion:\", response[\"emotion\"])\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã Chat interrupted. Exiting.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb87058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† MIRAGE AI (Terminal Mode)\n",
      "Type your message and press Enter.\n",
      "Type 'exit' to quit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_cli_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0089a54b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
